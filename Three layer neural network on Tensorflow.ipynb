{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import Tkinter as tk\n",
    "import tkFileDialog\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Preprocessing import load_images\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics as sk\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "np.random.seed(1337)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/edward/project /Dataset/Training_images Segmented\n"
     ]
    }
   ],
   "source": [
    "tk.Tk().withdraw()\n",
    "path_Train=tkFileDialog.askdirectory()\n",
    "print path_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train=load_images(path_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 64, 64, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train=np.array(X_Train)\n",
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 12288)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train=np.reshape(X_Train,[-1,np.prod(X_Train.shape[1:])])\n",
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/edward/project /Dataset/Testing_images Segmented\n"
     ]
    }
   ],
   "source": [
    "tk.Tk().withdraw()\n",
    "path_Test=tkFileDialog.askdirectory()\n",
    "print path_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_Test=load_images(path_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 64, 64, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test=np.array(X_Test)\n",
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 12288)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test=np.reshape(X_Test,[-1,np.prod(X_Test.shape[1:])])\n",
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/edward/project /Dataset/Validation_images Segmented\n"
     ]
    }
   ],
   "source": [
    "tk.Tk().withdraw()\n",
    "path_Val=tkFileDialog.askdirectory()\n",
    "print path_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 64, 64, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Val=load_images(path_Val)\n",
    "X_Val=np.array(X_Val)\n",
    "X_Val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 12288)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Val=np.reshape(X_Val,[-1,np.prod(X_Val.shape[1:])])\n",
    "X_Val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_Train=pd.read_csv(\"/home/edward/Skin Cancer Data/ISIC-2017_Training_Part3_GroundTruth.csv\")\n",
    "data_Test=pd.read_csv(\"/home/edward/Skin Cancer Data/ISIC-2017_Test_v2_Part3_GroundTruth.csv\")\n",
    "data_Val=pd.read_csv(\"/home/edward/Skin Cancer Data/ISIC-2017_Validation_Part3_GroundTruth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(600,)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "data_Train=data_Train.iloc[0:2000,1]\n",
    "data_Test=data_Test.iloc[0:600,1]\n",
    "data_Val=data_Val.iloc[0:150,1]\n",
    "print data_Train.shape\n",
    "print data_Test.shape\n",
    "print data_Val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_Train=data_Train\n",
    "y_test=data_Test\n",
    "y_Val=data_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_Train=np.array(y_Train)\n",
    "y_Test=np.array(y_test)\n",
    "y_Val=np.array(y_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_Train = np_utils.to_categorical(data_Train)\n",
    "y_Test = np_utils.to_categorical(data_Test)\n",
    "y_Val = np_utils.to_categorical(data_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.005\n",
    "training_epoch=15\n",
    "batch_size=100\n",
    "display_step=1\n",
    "model_path = \"/home/edward/Trained models/model_sig.ckpt\"\n",
    "n_hidden_1=256\n",
    "n_hidden_2=256\n",
    "n_input=64*64*3\n",
    "n_classes=2\n",
    "\n",
    "x=tf.placeholder(\"float\",[None,n_input])\n",
    "y=tf.placeholder(\"float\",[None,n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiplayer_perceptron(x,weights,biases):\n",
    "    layer_1=tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer_1=tf.nn.sigmoid(layer_1)\n",
    "    layer_2=tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
    "    layer_2=tf.nn.sigmoid(layer_2)\n",
    "    out_layer=tf.matmul(layer_2,weights['out'])+biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store layer weights and biases\n",
    "weights={\n",
    "    'h1':tf.Variable(tf.random_normal([n_input,n_hidden_1])),\n",
    "    'h2':tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal([n_hidden_1,n_classes]))\n",
    "}\n",
    "\n",
    "biases={\n",
    "    'b1':tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2':tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred=multiplayer_perceptron(x,weights,biases)\n",
    "\n",
    "# define loss and optimizer\n",
    "predictions=tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y)\n",
    "cost=tf.reduce_mean(predictions)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initilizing the variables\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model with sigmoid Activation function\n",
    "#saver_sigmoid=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.228664105\n",
      "Epoch: 0002 cost= 0.678387476\n",
      "Epoch: 0003 cost= 0.509715594\n",
      "Epoch: 0004 cost= 0.457714663\n",
      "Epoch: 0005 cost= 0.419929296\n",
      "Epoch: 0006 cost= 0.384350294\n",
      "Epoch: 0007 cost= 0.354540801\n",
      "Epoch: 0008 cost= 0.340923613\n",
      "Epoch: 0009 cost= 0.322776522\n",
      "Epoch: 0010 cost= 0.305304616\n",
      "Epoch: 0011 cost= 0.288729524\n",
      "Epoch: 0012 cost= 0.269727160\n",
      "Epoch: 0013 cost= 0.271325109\n",
      "Epoch: 0014 cost= 0.244329674\n",
      "Epoch: 0015 cost= 0.234174593\n",
      "Optimization Finished\n",
      "Training Accuracy: 0.94\n",
      "Testing Accuracy: 0.73\n",
      "Validation Accuracy: 0.67\n",
      "Precision 0.0645161290323\n",
      "Recall 0.017094017094\n",
      "confusion_matrix\n",
      "[[454  29]\n",
      " [115   2]]\n",
      "kappa score -0.0595246854536\n"
     ]
    }
   ],
   "source": [
    "# launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training \n",
    "    # uncomment below two line while you are restoring model\n",
    "    #load_path = saver_sigmoid.restore(sess, model_path)\n",
    "    #print \"Model restored from file: %s\" % save_path\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost=0\n",
    "        for i in range(batch_size):\n",
    "            train_idx=np.random.randint(X_Train.shape[0],size=100)\n",
    "            batch_x_Train=X_Train[train_idx,:]\n",
    "            batch_y_Train=y_Train[train_idx]\n",
    "            _,c=sess.run([optimizer,cost],feed_dict={x:batch_x_Train,y:batch_y_Train})\n",
    "            avg_cost+=c/batch_size\n",
    "            \n",
    "        if epoch%display_step==0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost)\n",
    "    print \"Optimization Finished\"\n",
    "    \n",
    "    Predictions=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(Predictions,\"float\"))\n",
    "    test_idx=np.random.randint(X_Test.shape[0],size=100)\n",
    "    batch_x_Test=X_Test[test_idx,:]\n",
    "    batch_y_Test=y_Test[test_idx]\n",
    "    val_idx=np.random.randint(X_Val.shape[0],size=100)\n",
    "    batch_x_Val=X_Val[val_idx,:]\n",
    "    batch_y_Val=y_Val[val_idx]\n",
    "    print \"Training Accuracy:\" ,accuracy.eval({x:batch_x_Train,y:batch_y_Train})\n",
    "    print \"Testing Accuracy:\" ,accuracy.eval({x:batch_x_Test,y:batch_y_Test})\n",
    "    print \"Validation Accuracy:\" ,accuracy.eval({x:batch_x_Val,y:batch_y_Val})\n",
    "    # comment below two while you are restore the model\n",
    "    #save_path=saver_sigmoid.save(sess,model_path)\n",
    "    #print \"Model saved in file: %s\" % save_path\n",
    "    y_p = tf.argmax(pred, 1)\n",
    "    val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x:X_Test, y:y_Test})\n",
    "    y_true = np.argmax(y_Test,1)\n",
    "    print \"Precision\", sk.precision_score(y_true, y_pred)\n",
    "    print \"Recall\", sk.recall_score(y_true, y_pred)\n",
    "    print \"confusion_matrix\"\n",
    "    print sk.confusion_matrix(y_true, y_pred)\n",
    "    print \"kappa score\",cohen_kappa_score(y_true,y_pred)\n",
    "    fpr, tpr, tresholds = sk.roc_curve(y_true, y_pred)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"/home/edward/Trained models/model_tanh.ckpt\"\n",
    "def multiplayer_perceptron_tanh(x,weights,biases):\n",
    "    layer_1=tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer_1=tf.nn.tanh(layer_1)\n",
    "    layer_2=tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
    "    layer_2=tf.nn.tanh(layer_2)\n",
    "    out_layer=tf.matmul(layer_2,weights['out'])+biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store layer weights and biases\n",
    "weights={\n",
    "    'h1':tf.Variable(tf.random_normal([n_input,n_hidden_1])),\n",
    "    'h2':tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal([n_hidden_1,n_classes]))\n",
    "}\n",
    "\n",
    "biases={\n",
    "    'b1':tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2':tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred=multiplayer_perceptron_tanh(x,weights,biases)\n",
    "\n",
    "# define loss and optimizer\n",
    "predictions=tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y)\n",
    "cost=tf.reduce_mean(predictions)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initilizing the variables\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model with sigmoid Activation function\n",
    "#saver_tanh=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 3.610109921\n",
      "Epoch: 0002 cost= 2.239949865\n",
      "Epoch: 0003 cost= 1.261066287\n",
      "Epoch: 0004 cost= 0.860407704\n",
      "Epoch: 0005 cost= 0.649729277\n",
      "Epoch: 0006 cost= 0.416621132\n",
      "Epoch: 0007 cost= 0.300595850\n",
      "Epoch: 0008 cost= 0.275609066\n",
      "Epoch: 0009 cost= 0.230118795\n",
      "Epoch: 0010 cost= 0.219035891\n",
      "Epoch: 0011 cost= 0.220502406\n",
      "Epoch: 0012 cost= 0.194644471\n",
      "Epoch: 0013 cost= 0.183065884\n",
      "Epoch: 0014 cost= 0.176744824\n",
      "Epoch: 0015 cost= 0.168487780\n",
      "Optimization Finished\n",
      "Training Accuracy: 0.95\n",
      "Testing Accuracy: 0.74\n",
      "Validation Accuracy: 0.81\n",
      "Precision 0.0851063829787\n",
      "Recall 0.034188034188\n",
      "confusion_matrix\n",
      "[[440  43]\n",
      " [113   4]]\n",
      "kappa score -0.0709137090684\n"
     ]
    }
   ],
   "source": [
    "# launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training \n",
    "    # uncomment below two line while you are restoring model\n",
    "    #load_path = saver_sigmoid.restore(sess, model_path)\n",
    "    #print \"Model restored from file: %s\" % save_path\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost=0\n",
    "        for i in range(batch_size):\n",
    "            train_idx=np.random.randint(X_Train.shape[0],size=100)\n",
    "            batch_x_Train=X_Train[train_idx,:]\n",
    "            batch_y_Train=y_Train[train_idx]\n",
    "            _,c=sess.run([optimizer,cost],feed_dict={x:batch_x_Train,y:batch_y_Train})\n",
    "            avg_cost+=c/batch_size\n",
    "            \n",
    "        if epoch%display_step==0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost)\n",
    "    print \"Optimization Finished\"\n",
    "    \n",
    "    Predictions=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(Predictions,\"float\"))\n",
    "    test_idx=np.random.randint(X_Test.shape[0],size=100)\n",
    "    batch_x_Test=X_Test[test_idx,:]\n",
    "    batch_y_Test=y_Test[test_idx]\n",
    "    val_idx=np.random.randint(X_Val.shape[0],size=100)\n",
    "    batch_x_Val=X_Val[val_idx,:]\n",
    "    batch_y_Val=y_Val[val_idx]\n",
    "    print \"Training Accuracy:\" ,accuracy.eval({x:batch_x_Train,y:batch_y_Train})\n",
    "    print \"Testing Accuracy:\" ,accuracy.eval({x:batch_x_Test,y:batch_y_Test})\n",
    "    print \"Validation Accuracy:\" ,accuracy.eval({x:batch_x_Val,y:batch_y_Val})\n",
    "    y_p = tf.argmax(pred, 1)\n",
    "    val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x:X_Test, y:y_Test})\n",
    "    y_true = np.argmax(y_Test,1)\n",
    "    print \"Precision\", sk.precision_score(y_true, y_pred)\n",
    "    print \"Recall\", sk.recall_score(y_true, y_pred)\n",
    "    print \"confusion_matrix\"\n",
    "    print sk.confusion_matrix(y_true, y_pred)\n",
    "    print \"kappa score\",cohen_kappa_score(y_true,y_pred)\n",
    "    fpr, tpr, tresholds = sk.roc_curve(y_true, y_pred)\n",
    "    # comment below two while you are restore the model\n",
    "    #save_path=saver_tanh.save(sess,model_path)\n",
    "    #print \"Model saved in file: %s\" % save_path\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"/home/edward/Trained models/model_relu.ckpt\"\n",
    "def multiplayer_perceptron_relu(x,weights,biases):\n",
    "    layer_1=tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer_1=tf.nn.relu(layer_1)\n",
    "    layer_2=tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
    "    layer_2=tf.nn.relu(layer_2)\n",
    "    out_layer=tf.matmul(layer_2,weights['out'])+biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store layer weights and biases\n",
    "weights={\n",
    "    'h1':tf.Variable(tf.random_normal([n_input,n_hidden_1])),\n",
    "    'h2':tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal([n_hidden_1,n_classes]))\n",
    "}\n",
    "\n",
    "biases={\n",
    "    'b1':tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2':tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred=multiplayer_perceptron_relu(x,weights,biases)\n",
    "\n",
    "# define loss and optimizer\n",
    "predictions=tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y)\n",
    "cost=tf.reduce_mean(predictions)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initilizing the variables\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model with sigmoid Activation function\n",
    "#saver_relu=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 59548.488125000\n",
      "Epoch: 0002 cost= 8611.800339966\n",
      "Epoch: 0003 cost= 2458.260316582\n",
      "Epoch: 0004 cost= 2429.383820686\n",
      "Epoch: 0005 cost= 1068.878244543\n",
      "Epoch: 0006 cost= 1094.142381911\n",
      "Epoch: 0007 cost= 2241.816273003\n",
      "Epoch: 0008 cost= 1826.386764469\n",
      "Epoch: 0009 cost= 1576.241149607\n",
      "Epoch: 0010 cost= 887.070541590\n",
      "Epoch: 0011 cost= 539.564180584\n",
      "Epoch: 0012 cost= 694.889221716\n",
      "Epoch: 0013 cost= 1150.702815208\n",
      "Epoch: 0014 cost= 1830.847847433\n",
      "Epoch: 0015 cost= 1874.240559444\n",
      "Optimization Finished\n",
      "Training Accuracy: 0.99\n",
      "Testing Accuracy: 0.72\n",
      "Validation Accuracy: 0.69\n",
      "Precision 0.130434782609\n",
      "Recall 0.0769230769231\n",
      "confusion_matrix\n",
      "[[423  60]\n",
      " [108   9]]\n",
      "kappa score -0.0560060343202\n"
     ]
    }
   ],
   "source": [
    "# launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training \n",
    "    # uncomment below two line while you are restoring model\n",
    "    #load_path = saver_sigmoid.restore(sess, model_path)\n",
    "    #print \"Model restored from file: %s\" % save_path\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost=0\n",
    "        for i in range(batch_size):\n",
    "            train_idx=np.random.randint(X_Train.shape[0],size=100)\n",
    "            batch_x_Train=X_Train[train_idx,:]\n",
    "            batch_y_Train=y_Train[train_idx]\n",
    "            _,c=sess.run([optimizer,cost],feed_dict={x:batch_x_Train,y:batch_y_Train})\n",
    "            avg_cost+=c/batch_size\n",
    "            \n",
    "        if epoch%display_step==0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost)\n",
    "    print \"Optimization Finished\"\n",
    "    \n",
    "    Predictions=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(Predictions,\"float\"))\n",
    "    test_idx=np.random.randint(X_Test.shape[0],size=100)\n",
    "    batch_x_Test=X_Test[test_idx,:]\n",
    "    batch_y_Test=y_Test[test_idx]\n",
    "    val_idx=np.random.randint(X_Val.shape[0],size=100)\n",
    "    batch_x_Val=X_Val[val_idx,:]\n",
    "    batch_y_Val=y_Val[val_idx]\n",
    "    print \"Training Accuracy:\" ,accuracy.eval({x:batch_x_Train,y:batch_y_Train})\n",
    "    print \"Testing Accuracy:\" ,accuracy.eval({x:batch_x_Test,y:batch_y_Test})\n",
    "    print \"Validation Accuracy:\" ,accuracy.eval({x:batch_x_Val,y:batch_y_Val})\n",
    "    y_p = tf.argmax(pred, 1)\n",
    "    val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x:X_Test, y:y_Test})\n",
    "    y_true = np.argmax(y_Test,1)\n",
    "    print \"Precision\", sk.precision_score(y_true, y_pred)\n",
    "    print \"Recall\", sk.recall_score(y_true, y_pred)\n",
    "    print \"confusion_matrix\"\n",
    "    print sk.confusion_matrix(y_true, y_pred)\n",
    "    fpr, tpr, tresholds = sk.roc_curve(y_true, y_pred)\n",
    "    print \"kappa score\",cohen_kappa_score(y_true,y_pred)\n",
    "    # comment below two while you are restore the model\n",
    "    #save_path=saver_relu.save(sess,model_path)\n",
    "    #print \"Model saved in file: %s\" % save_path\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
