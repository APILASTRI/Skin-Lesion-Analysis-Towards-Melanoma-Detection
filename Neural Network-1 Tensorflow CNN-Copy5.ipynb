{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import Tkinter as tk\n",
    "import tkFileDialog\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Preprocessing import load_images\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics as sk\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "np.random.seed(200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/edward/project /Dataset/Training Images\n"
     ]
    }
   ],
   "source": [
    "tk.Tk().withdraw()\n",
    "path_Train=tkFileDialog.askdirectory()\n",
    "print path_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_Train=load_images(path_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 64, 64, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train=np.array(X_Train)\n",
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_Train=np.reshape(X_Train,[-1,np.prod(X_Train.shape[1:])])\n",
    "#X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/edward/project /Dataset/Testing Images\n"
     ]
    }
   ],
   "source": [
    "tk.Tk().withdraw()\n",
    "path_Test=tkFileDialog.askdirectory()\n",
    "print path_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_Test=load_images(path_Test)\n",
    "X_Test=np.array(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_Test=np.reshape(X_Test,[-1,np.prod(X_Test.shape[1:])])\n",
    "#X_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_Train=pd.read_csv(\"/home/edward/Skin Cancer Data/ISIC-2017_Training_Part3_GroundTruth.csv\")\n",
    "data_Test=pd.read_csv(\"/home/edward/Skin Cancer Data/ISIC-2017_Test_v2_Part3_GroundTruth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "data_Train=data_Train.iloc[0:2000,1]\n",
    "data_Test=data_Test.iloc[0:600,1]\n",
    "print data_Train.shape\n",
    "print data_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_Train=data_Train\n",
    "y_Test=data_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_Train=np.array(y_Train)\n",
    "y_Test=np.array(y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "(600, 2)\n"
     ]
    }
   ],
   "source": [
    "y_Train = np_utils.to_categorical(data_Train)\n",
    "y_Test = np_utils.to_categorical(data_Test)\n",
    "print y_Train.shape\n",
    "print y_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/edward/project /Dataset/Validation Images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 64, 64, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.Tk().withdraw()\n",
    "path_Validation=tkFileDialog.askdirectory()\n",
    "print path_Validation\n",
    "X_Val=load_images(path_Validation)\n",
    "X_Val=np.array(X_Val)\n",
    "X_Val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "y_Val=pd.read_csv(\"/home/edward/Skin Cancer Data/ISIC-2017_Validation_Part3_GroundTruth.csv\")\n",
    "Data_Validation=y_Val.iloc[0:150,1]\n",
    "print y_Val.shape\n",
    "y_Val = np_utils.to_categorical(Data_Validation)\n",
    "print y_Val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_normalize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "X_Train=center_normalize(X_Train)\n",
    "X_Test=center_normalize(X_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholders and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "img_size=64\n",
    "num_channels=3\n",
    "num_classes=2\n",
    "img_size_flat=img_size*img_size*num_channels\n",
    "with tf.name_scope(\"Input\"):\n",
    "    x=tf.placeholder(tf.float32,shape=[None,img_size_flat],name='x')\n",
    "    x=tf.reshape(x,[-1,img_size,img_size,num_channels])\n",
    "logs_path = '/tmp/tensorflow_logs/example'\n",
    "\n",
    "y_true=tf.placeholder(tf.float32,shape=[None,num_classes],name='y_true')\n",
    "y_true_cls=tf.argmax(y_true,dimension=1)\n",
    "dropout=0.75\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob=tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "\n",
    "filter_size1=3\n",
    "num_filters1=16\n",
    "filter_size2=3\n",
    "num_filters2=16\n",
    "fc_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initilization of Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_initilization(shape):\n",
    "    return tf.Variable(tf.random_normal(shape,stddev=0.05))\n",
    "def biases_initilization(length):\n",
    "    return tf.Variable(tf.random_normal(shape=[length]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution + pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input,input_channels,filter_size,num_filters,layer_name,use_pooling=True):\n",
    "    with tf.name_scope(layer_name):\n",
    "        shape=[filter_size,filter_size,input_channels,num_filters]\n",
    "        with tf.name_scope('Weights'):\n",
    "            weights=weights_initilization(shape)\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope(\"Biases\"):\n",
    "            biases=biases_initilization(length=num_filters)\n",
    "            variable_summaries(biases)\n",
    "        conv_layer=tf.nn.conv2d(input=input,filter=weights,strides=[1,1,1,1],padding='SAME')\n",
    "        conv_layer+=biases\n",
    "\n",
    "        if use_pooling:\n",
    "            pooling_layer=tf.nn.max_pool(value=conv_layer,ksize=[1,2,2,1],\n",
    "                                        strides=[1,2,2,1],padding='SAME')\n",
    "        Activation_layer=tf.nn.relu(pooling_layer)\n",
    "    return Activation_layer,weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer,layer_name):\n",
    "    with tf.name_scope(layer_name):\n",
    "        layer_shape=layer.get_shape()\n",
    "        num_features=layer_shape[1:4].num_elements()\n",
    "        layer_flat=tf.reshape(layer,[-1,num_features])\n",
    "    return layer_flat,num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(input,num_inputs,num_outputs,layer_name,keep_prob,use_relu=True):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope(\"Weights_FC\"):\n",
    "            weights=weights_initilization(shape=[num_inputs,num_outputs])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope(\"biases_FC\"):\n",
    "            biases=biases_initilization(length=num_outputs)\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope(\"Wx_plus_b\"):\n",
    "            layer=tf.matmul(input,weights)+biases\n",
    "            tf.summary.histogram('Preactivation',layer)\n",
    "        with tf.name_scope('Activation'):\n",
    "            if use_relu:\n",
    "                layer=tf.nn.relu(layer)\n",
    "                tf.summary.histogram('activation',layer)\n",
    "        with tf.name_scope('dropout'):\n",
    "            layer=tf.nn.dropout(layer,keep_prob)\n",
    "    return layer,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv1,weights_conv1=conv_layer(input=x,input_channels=num_channels,filter_size=filter_size1,\n",
    "                                     num_filters=num_filters1,layer_name='layer1',use_pooling=True)\n",
    "layer_conv2,weights_conv2=conv_layer(input=layer_conv1,input_channels=num_filters1,filter_size=filter_size2,\n",
    "                                     num_filters=num_filters2,use_pooling=True,layer_name='layer2')\n",
    "layer_conv3,weights_conv3=conv_layer(input=layer_conv1,input_channels=num_filters1,filter_size=filter_size2,\n",
    "                                     num_filters=num_filters2,use_pooling=True,layer_name='layer3')\n",
    "layer_flat,num_features=flatten_layer(layer_conv3,'layer4')\n",
    "layer_fc1,weights_FC1=fc_layer(input=layer_flat,num_inputs=num_features,num_outputs=num_classes,keep_prob=keep_prob,layer_name='layer5',use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.variables.Variable at 0x7f6ad9be2b90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_FC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cost:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=tf.nn.softmax(layer_fc1)\n",
    "y_pred_cls=tf.argmax(y_pred,dimension=1)\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy=tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc1,labels=y_true)\n",
    "with tf.name_scope(\"total\"):  \n",
    "    cost=tf.reduce_mean(cross_entropy)\n",
    "tf.summary.scalar('cost',cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('train'):\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cross_entropy)\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction=tf.equal(y_pred_cls,y_true_cls)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "#tf.summary.scalar('correct_prediction',correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "merged=tf.summary.merge_all()\n",
    "train_writer=tf.summary.FileWriter(logs_path+'/train',sess.graph)\n",
    "test_writer=tf.summary.FileWriter(logdir=logs_path+\"/test\")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "with sess.as_default():\n",
    "    tf.global_variables_initializer().run()\n",
    "train_batch_size=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_progress(epoch,feed_dict_train,feed_dict_test,test_loss):\n",
    "    acc=sess.run(accuracy,feed_dict=feed_dict_train)\n",
    "    test_acc=sess.run(accuracy,feed_dict=feed_dict_test)\n",
    "    \n",
    "    msg=\"Epoch {0} --- Training Accuracy:{1:>6.1%},Testing Accuracy:{2:>6.1%}, Test Loss:{3:.3f}\"\n",
    "    print msg.format(epoch+1,acc,test_acc,test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_iterations=0\n",
    "batch_size=100\n",
    "img_size_flat=img_size*img_size*num_channels\n",
    "def optimize(num_iterations):\n",
    "    global total_iterations\n",
    "    best_test_loss=float(\"inf\")\n",
    "    summary_writer=tf.summary.FileWriter(logs_path,graph=sess.graph)\n",
    "    for i in range(total_iterations,total_iterations+num_iterations):\n",
    "        train_idx=np.random.randint(X_Train.shape[0],size=100)\n",
    "        batch_x_Train=X_Train[train_idx,:]\n",
    "        batch_y_Train=y_Train[train_idx]\n",
    "        #batch_x_Train=batch_x_Train.reshape(batch_size,img_size_flat)\n",
    "        train_idx=np.random.randint(X_Test.shape[0],size=100)\n",
    "        batch_x_Test=X_Test[train_idx,:]\n",
    "        batch_y_Test=y_Test[train_idx]\n",
    "        #batch_x_Test=batch_x_Test.reshape(batch_size,img_size_flat)\n",
    "        feed_dict_train = {x: batch_x_Train,\n",
    "                           y_true: batch_y_Train,keep_prob:dropout}\n",
    "        \n",
    "        feed_dict_test = {x: batch_x_Test,\n",
    "                              y_true: batch_y_Test,keep_prob:1.0}\n",
    "        _,c,summary=sess.run([optimizer,cost,merged],feed_dict=feed_dict_train)\n",
    "        if i% int(X_Train.shape[0]/batch_size)==0:\n",
    "            test_loss=sess.run(cost,feed_dict=feed_dict_test)\n",
    "            epoch=int(i/(X_Train.shape[0]/batch_size))\n",
    "            summary_writer.add_summary(summary,epoch*batch_size+i)\n",
    "            print_progress(epoch,feed_dict_train,feed_dict_test,test_loss)\n",
    "        total_iterations+=num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --- Training Accuracy: 77.0%,Testing Accuracy: 78.0%, Test Loss:0.540\n",
      "Epoch 2 --- Training Accuracy: 75.0%,Testing Accuracy: 76.0%, Test Loss:0.549\n",
      "Epoch 3 --- Training Accuracy: 79.0%,Testing Accuracy: 86.0%, Test Loss:0.433\n",
      "Epoch 4 --- Training Accuracy: 80.0%,Testing Accuracy: 79.0%, Test Loss:0.506\n",
      "Epoch 5 --- Training Accuracy: 81.0%,Testing Accuracy: 78.0%, Test Loss:0.529\n",
      "Epoch 6 --- Training Accuracy: 79.0%,Testing Accuracy: 84.0%, Test Loss:0.466\n",
      "Epoch 7 --- Training Accuracy: 77.0%,Testing Accuracy: 74.0%, Test Loss:0.559\n",
      "Epoch 8 --- Training Accuracy: 89.0%,Testing Accuracy: 74.0%, Test Loss:0.551\n",
      "Epoch 9 --- Training Accuracy: 80.0%,Testing Accuracy: 85.0%, Test Loss:0.489\n",
      "Epoch 10 --- Training Accuracy: 86.0%,Testing Accuracy: 80.0%, Test Loss:0.504\n",
      "Epoch 11 --- Training Accuracy: 81.0%,Testing Accuracy: 74.0%, Test Loss:0.562\n",
      "Epoch 12 --- Training Accuracy: 81.0%,Testing Accuracy: 76.0%, Test Loss:0.541\n",
      "Epoch 13 --- Training Accuracy: 85.0%,Testing Accuracy: 87.0%, Test Loss:0.406\n",
      "Epoch 14 --- Training Accuracy: 76.0%,Testing Accuracy: 82.0%, Test Loss:0.468\n",
      "Epoch 15 --- Training Accuracy: 82.0%,Testing Accuracy: 81.0%, Test Loss:0.475\n",
      "Epoch 16 --- Training Accuracy: 80.0%,Testing Accuracy: 81.0%, Test Loss:0.492\n",
      "Epoch 17 --- Training Accuracy: 82.0%,Testing Accuracy: 74.0%, Test Loss:0.549\n",
      "Epoch 18 --- Training Accuracy: 86.0%,Testing Accuracy: 78.0%, Test Loss:0.532\n",
      "Epoch 19 --- Training Accuracy: 89.0%,Testing Accuracy: 80.0%, Test Loss:0.497\n",
      "Epoch 20 --- Training Accuracy: 81.0%,Testing Accuracy: 85.0%, Test Loss:0.436\n",
      "Epoch 21 --- Training Accuracy: 78.0%,Testing Accuracy: 83.0%, Test Loss:0.466\n",
      "Epoch 22 --- Training Accuracy: 83.0%,Testing Accuracy: 83.0%, Test Loss:0.474\n",
      "Epoch 23 --- Training Accuracy: 81.0%,Testing Accuracy: 75.0%, Test Loss:0.565\n",
      "Epoch 24 --- Training Accuracy: 80.0%,Testing Accuracy: 81.0%, Test Loss:0.473\n",
      "Epoch 25 --- Training Accuracy: 86.0%,Testing Accuracy: 82.0%, Test Loss:0.476\n",
      "Epoch 26 --- Training Accuracy: 78.0%,Testing Accuracy: 81.0%, Test Loss:0.480\n",
      "Epoch 27 --- Training Accuracy: 77.0%,Testing Accuracy: 73.0%, Test Loss:0.579\n",
      "Epoch 28 --- Training Accuracy: 85.0%,Testing Accuracy: 78.0%, Test Loss:0.499\n",
      "Epoch 29 --- Training Accuracy: 86.0%,Testing Accuracy: 77.0%, Test Loss:0.516\n",
      "Epoch 30 --- Training Accuracy: 80.0%,Testing Accuracy: 86.0%, Test Loss:0.425\n",
      "Epoch 31 --- Training Accuracy: 81.0%,Testing Accuracy: 80.0%, Test Loss:0.501\n",
      "Epoch 32 --- Training Accuracy: 79.0%,Testing Accuracy: 76.0%, Test Loss:0.539\n",
      "Epoch 33 --- Training Accuracy: 77.0%,Testing Accuracy: 75.0%, Test Loss:0.559\n",
      "Epoch 34 --- Training Accuracy: 80.0%,Testing Accuracy: 87.0%, Test Loss:0.441\n",
      "Epoch 35 --- Training Accuracy: 89.0%,Testing Accuracy: 84.0%, Test Loss:0.458\n",
      "Epoch 36 --- Training Accuracy: 80.0%,Testing Accuracy: 78.0%, Test Loss:0.501\n",
      "Epoch 37 --- Training Accuracy: 75.0%,Testing Accuracy: 83.0%, Test Loss:0.464\n",
      "Epoch 38 --- Training Accuracy: 79.0%,Testing Accuracy: 78.0%, Test Loss:0.525\n",
      "Epoch 39 --- Training Accuracy: 79.0%,Testing Accuracy: 87.0%, Test Loss:0.416\n",
      "Epoch 40 --- Training Accuracy: 79.0%,Testing Accuracy: 84.0%, Test Loss:0.462\n",
      "Epoch 41 --- Training Accuracy: 83.0%,Testing Accuracy: 82.0%, Test Loss:0.474\n",
      "Epoch 42 --- Training Accuracy: 87.0%,Testing Accuracy: 80.0%, Test Loss:0.493\n",
      "Epoch 43 --- Training Accuracy: 81.0%,Testing Accuracy: 79.0%, Test Loss:0.528\n",
      "Epoch 44 --- Training Accuracy: 88.0%,Testing Accuracy: 73.0%, Test Loss:0.578\n",
      "Epoch 45 --- Training Accuracy: 83.0%,Testing Accuracy: 76.0%, Test Loss:0.557\n",
      "Epoch 46 --- Training Accuracy: 78.0%,Testing Accuracy: 81.0%, Test Loss:0.493\n",
      "Epoch 47 --- Training Accuracy: 79.0%,Testing Accuracy: 82.0%, Test Loss:0.494\n",
      "Epoch 48 --- Training Accuracy: 84.0%,Testing Accuracy: 81.0%, Test Loss:0.519\n",
      "Epoch 49 --- Training Accuracy: 77.0%,Testing Accuracy: 75.0%, Test Loss:0.548\n",
      "Epoch 50 --- Training Accuracy: 83.0%,Testing Accuracy: 86.0%, Test Loss:0.458\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
