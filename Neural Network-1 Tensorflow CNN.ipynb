{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import Tkinter as tk\n",
    "import tkFileDialog\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Preprocessing import load_images\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics as sk\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "np.random.seed(200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akash/Skin-Lesion-Analysis-Towards-Melanoma-Detection/Training Images\n"
     ]
    }
   ],
   "source": [
    "tk.Tk().withdraw()\n",
    "path_Train=tkFileDialog.askdirectory()\n",
    "print path_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train=load_images(path_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 64, 64, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train=np.array(X_Train)\n",
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 12288)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train=np.reshape(X_Train,[-1,np.prod(X_Train.shape[1:])])\n",
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akash/Skin-Lesion-Analysis-Towards-Melanoma-Detection/Testing Images\n"
     ]
    }
   ],
   "source": [
    "tk.Tk().withdraw()\n",
    "path_Test=tkFileDialog.askdirectory()\n",
    "print path_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 64, 64, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test=load_images(path_Test)\n",
    "X_Test=np.array(X_Test)\n",
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 12288)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test=np.reshape(X_Test,[-1,np.prod(X_Test.shape[1:])])\n",
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_Train=pd.read_csv(\"/home/akash/Skin-Lesion-Analysis-Towards-Melanoma-Detection/ISIC-2017_Training_Part3_GroundTruth.csv\")\n",
    "data_Test=pd.read_csv(\"/home/akash/Skin-Lesion-Analysis-Towards-Melanoma-Detection/ISIC-2017_Test_v2_Part3_GroundTruth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "data_Train=data_Train.iloc[0:2000,1]\n",
    "data_Test=data_Test.iloc[0:600,1]\n",
    "print data_Train.shape\n",
    "print data_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_Train=data_Train\n",
    "y_Test=data_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_Train=np.array(y_Train)\n",
    "y_Test=np.array(y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "(600, 2)\n"
     ]
    }
   ],
   "source": [
    "y_Train = np_utils.to_categorical(data_Train)\n",
    "y_Test = np_utils.to_categorical(data_Test)\n",
    "print y_Train.shape\n",
    "print y_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_normalize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "X_Train=center_normalize(X_Train)\n",
    "X_Test=center_normalize(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 12288)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "training_iterations=50000\n",
    "batch_size=100\n",
    "display_step=10\n",
    "logs_path = '/tmp/tensorflow_logs/example'\n",
    "n_input=64*64*3 # Data input \n",
    "n_classes=2 # Classes\n",
    "dropout=0.75\n",
    "\n",
    "x=tf.placeholder(\"float\",[None,n_input])\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "Drop_prob=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 64, 64, 3])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 3, 16])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 16, 16])),\n",
    "    # fully connected, 16*16*64 inputs, 4096 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([16*16*16, 12288])),\n",
    "    # 4096 inputs, 3 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([12288, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([16])),\n",
    "    'bc2': tf.Variable(tf.random_normal([16])),\n",
    "    'bd1': tf.Variable(tf.random_normal([12288])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "with tf.name_scope(\"Model\"):\n",
    "    pred= conv_net(x, weights, biases, Drop_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope(\"loss\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "with tf.name_scope(\"Adam\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# summaries\n",
    "tf.summary.scalar(\"loss\",cost)\n",
    "tf.summary.scalar(\"accuracy\",accuracy)\n",
    "merged_summary_op=tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:1000,Minibatch Loss=92402.851562,Training Accuracy=0.66000\n",
      "iters:2000,Minibatch Loss=15235.757812,Training Accuracy=0.81000\n",
      "iters:3000,Minibatch Loss=13821.407227,Training Accuracy=0.85000\n",
      "iters:4000,Minibatch Loss=5860.616211,Training Accuracy=0.86000\n",
      "iters:5000,Minibatch Loss=4305.635254,Training Accuracy=0.81000\n",
      "iters:6000,Minibatch Loss=1346.981689,Training Accuracy=0.89000\n",
      "iters:7000,Minibatch Loss=2519.657715,Training Accuracy=0.85000\n",
      "iters:8000,Minibatch Loss=1113.013062,Training Accuracy=0.92000\n",
      "iters:9000,Minibatch Loss=1483.367676,Training Accuracy=0.91000\n",
      "iters:10000,Minibatch Loss=536.172119,Training Accuracy=0.93000\n",
      "iters:11000,Minibatch Loss=458.709290,Training Accuracy=0.94000\n",
      "iters:12000,Minibatch Loss=382.804779,Training Accuracy=0.96000\n",
      "iters:13000,Minibatch Loss=1055.420410,Training Accuracy=0.94000\n",
      "iters:14000,Minibatch Loss=81.320709,Training Accuracy=0.98000\n",
      "iters:15000,Minibatch Loss=63.781754,Training Accuracy=0.99000\n",
      "iters:16000,Minibatch Loss=73.752792,Training Accuracy=0.99000\n",
      "iters:17000,Minibatch Loss=181.853165,Training Accuracy=0.99000\n",
      "iters:18000,Minibatch Loss=46.176682,Training Accuracy=0.98000\n",
      "iters:19000,Minibatch Loss=42.529709,Training Accuracy=0.99000\n",
      "iters:20000,Minibatch Loss=104.017113,Training Accuracy=0.99000\n",
      "iters:21000,Minibatch Loss=42.228115,Training Accuracy=0.99000\n",
      "iters:22000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:23000,Minibatch Loss=241.332932,Training Accuracy=0.97000\n",
      "iters:24000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:25000,Minibatch Loss=18.493731,Training Accuracy=0.99000\n",
      "iters:26000,Minibatch Loss=53.635475,Training Accuracy=0.98000\n",
      "iters:27000,Minibatch Loss=19.854980,Training Accuracy=0.99000\n",
      "iters:28000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:29000,Minibatch Loss=32.208458,Training Accuracy=0.99000\n",
      "iters:30000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:31000,Minibatch Loss=17.683222,Training Accuracy=0.99000\n",
      "iters:32000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:33000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:34000,Minibatch Loss=2.851611,Training Accuracy=0.99000\n",
      "iters:35000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:36000,Minibatch Loss=15.191192,Training Accuracy=0.99000\n",
      "iters:37000,Minibatch Loss=125.961159,Training Accuracy=0.99000\n",
      "iters:38000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:39000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:40000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:41000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:42000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:43000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:44000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:45000,Minibatch Loss=292.560730,Training Accuracy=0.99000\n",
      "iters:46000,Minibatch Loss=22.302757,Training Accuracy=0.99000\n",
      "iters:47000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:48000,Minibatch Loss=0.000000,Training Accuracy=1.00000\n",
      "iters:49000,Minibatch Loss=8.424751,Training Accuracy=0.99000\n",
      "Optimization Finished\n",
      "Testing accuracy 0.77\n"
     ]
    }
   ],
   "source": [
    "# launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step=1\n",
    "    # Training \n",
    "    summary_writer=tf.summary.FileWriter(logdir=logs_path,graph=tf.get_default_graph())\n",
    "    while step*batch_size<training_iterations:\n",
    "        train_idx=np.random.randint(X_Train.shape[0],size=batch_size)\n",
    "        batch_x_Train=X_Train[train_idx,:]\n",
    "        batch_y_Train=y_Train[train_idx]\n",
    "        _,c,summary=sess.run([optimizer,cost,merged_summary_op],feed_dict={x:batch_x_Train,y:batch_y_Train,Drop_prob:dropout})\n",
    "        summary_writer.add_summary(summary,step*batch_size)\n",
    "        if step%display_step==0:\n",
    "            loss,acc=sess.run([cost,accuracy],feed_dict={x:batch_x_Train,y:batch_y_Train,Drop_prob:1.})\n",
    "        \n",
    "            print \"iters:\"+ str(step*batch_size)+\",Minibatch Loss=\" + \\\n",
    "                \"{:.6f}\".format(loss)+\",Training Accuracy=\"+\\\n",
    "                \"{:.5f}\".format(acc)\n",
    "        step+=1\n",
    "    print \"Optimization Finished\"\n",
    "    test_idx=np.random.randint(X_Test.shape[0],size=batch_size)\n",
    "    batch_x_Test=X_Test[test_idx,:]\n",
    "    batch_y_Test=y_Test[test_idx]\n",
    "    print \"Testing accuracy\",\\\n",
    "        sess.run(accuracy,feed_dict={x:batch_x_Test,y:batch_y_Test,Drop_prob:1.})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
